# Course AI-900T00-A: Microsoft Azure AI Fundamentals

[Course AI-900T00-A: Microsoft Azure AI Fundamentals - Training | Microsoft Learn](https://learn.microsoft.com/en-us/training/courses/ai-900t00)

- **Microsoft Azure AI Fundamentals: AI Overview**
    - **Fundamental AI Concepts**
        - **Theory**
            1. Simply put, AI is software that imitates **human behaviors and capabilities**. Key workloads include:
                1. **Machine learning** - This is often the foundation for an AI system, and is the way we "teach" a computer model to make predictions and draw conclusions from data.
                2. **Computer vision** - Capabilities within AI to interpret the world visually through cameras, video, and images.
                3. **Natural language processing** - Capabilities within AI for a computer to interpret written or spoken language, and respond in kind.
                4. **Document intelligence** - Capabilities within AI that deal with managing, processing, and using high volumes of data found in forms and documents.
                5. **Knowledge mining** - Capabilities within AI to extract information from large volumes of often unstructured data to create a searchable knowledge store.
                6. **Generative AI** - Capabilities within AI that create original content in a variety of formats including natural language, image, code, and more.
            2. **Machine learning in Microsoft Azure -** Microsoft Azure provides the **Azure Machine Learning** service - a cloud-based platform for creating, managing, and publishing machine learning models. **Azure Machine Learning Studio** offers multiple authoring experiences such as:
                1. **Automated machine learning:** this feature enables non-experts to quickly create an effective machine learning model from data.
                2. **Azure Machine Learning designer:** a graphical interface enabling no-code development of machine learning solutions.
                3. **Data metric visualization:** analyze and optimize your experiments with visualization.
                4. **Notebooks**: write and run your own code in managed Jupyter Notebook servers that are directly integrated in the studio.
            3. **Computer vision services in Microsoft Azure -** You can use Microsoft's **Azure AI Vision** to develop computer vision solutions. The service features are available for use and testing in the **Azure Vision Studio** and other programming languages. Some features of Azure AI Vision include:
                1. **Image Analysis:** capabilities for analyzing images and video, and extracting descriptions, tags, objects, and text.
                2. **Face**: capabilities that enable you to build face detection and facial recognition solutions.
                3. **Optical Character Recognition (OCR)**: capabilities for extracting printed or handwritten text from images, enabling access to a digital version of the scanned text.
            4. **Understand natural language processing** 
                1. You can use Microsoft's **Azure AI Language** to build natural language processing solutions. Some features of Azure AI Language include understanding and analyzing text, training conversational language models that can understand spoken or text-based commands, and building intelligent applications.
                2. Microsoft's **Azure AI Speech** is another service that can be used to build natural language processing solutions. Azure AI Speech features include speech recognition and synthesis, real-time translations, conversation transcriptions, and more.
                3. You can explore Azure AI Language features in the **Azure Language Studio** and Azure AI Speech features in the **Azure Speech Studio**. The service features are available for use and testing in the studios and other programming languages.
            5. **Understand document intelligence and knowledge mining**
                1. **Document intelligence in Microsoft Azure -** You can use Microsoft's **Azure AI Document Intelligence** to build solutions that manage and accelerate data collection from scanned documents. Features of Azure AI Document Intelligence help automate document processing in applications and workflows, enhance data-driven strategies, and enrich document search capabilities. You can use prebuilt models to add intelligent document processing for invoices, receipts, health insurance cards, tax forms, and more. You can also use Azure AI Document Intelligence to create custom models with your own labeled datasets. The service features are available for use and testing in the **Document Intelligence Studio** and other programming languages.
                2. **Knowledge mining in Microsoft Azure -** One Microsoft knowledge mining solution is **Azure AI Search**, an enterprise, search solution that has tools for building indexes. The indexes can then be used for internal only use, or to enable searchable content on public facing internet assets. Azure AI Search can utilize the built-in AI capabilities of Azure AI services such as image processing, document intelligence, and natural language processing to extract data. The product's AI capabilities makes it possible to index previously unsearchable documents and to extract and surface insights from large amounts of data quickly.
            6. **Understand generative AI**
                1. **Generative AI in Microsoft Azure - Azure OpenAI Service** is Microsoft's cloud solution for deploying, customizing, and hosting generative AI models. It brings together the best of OpenAI's cutting edge models and APIs with the security and scalability of the Azure cloud platform. Azure OpenAI Service supports many generative model choices that can serve different needs. You can use **Azure AI Studio** to create generative AI solutions, such as custom *copilot* chat-based assistants that use Azure OpenAI Service models
            7. **Challenges and risks with AI**
                
                
                | Challenge or Risk | Example |
                | --- | --- |
                | Bias can affect results | A loan-approval model discriminates by gender due to bias in the data with which it was trained |
                | Errors may cause harm | An autonomous vehicle experiences a system failure and causes a collision |
                | Data could be exposed | A medical diagnostic bot is trained using sensitive patient data, which is stored insecurely |
                | Solutions may not work for everyone | A home automation assistant provides no audio output for visually impaired users |
                | Users must trust a complex system | An AI-based financial tool makes investment recommendations - what are they based on? |
                | Who's liable for AI-driven decisions? | An innocent person is convicted of a crime based on evidence from facial recognition – who's responsible? |
            8. **Understand Responsible AI**
                1. **Fairness**
                2. **Reliability and safety**
                3. **Privacy and security**
                4. **Inclusiveness**
                5. **Transparency**
                6. **Accountability**
        - **Knowledge check**
            1. You want to create a model to predict sales of ice cream based on historic data that includes daily ice cream sales totals and weather measurements. Which Azure service should you use? **- Azure Machine Learning. That is correct. Azure Machine Learning enables you to train a predictive model from the existing data.**
            2. You work for a wildlife sanctuary and are considering using AI to identify bird species from images. Which AI service should you use to prototype your idea? **- Azure AI Vision. Correct. Azure AI Vision allows you to add images to the existing model to improve the image identifier model. This is a good choice for identifying the small differences between bird species.**
            3. A predictive app provides audio output for visually impaired users. Which principle of Responsible AI is reflected here? **- Inclusiveness. That is correct. Inclusiveness is about how AI should bring benefits to all parts of society, regardless of physical ability, gender, sexual orientation, ethnicity, or other factors.**
    - **Fundamentals of machine learning**
        - **Theory**
            1. The fundamental idea of **machine learning is to use data from past observations to predict unknown outcomes or values.** For example
                1. Ice cream sales
                2. Risk from diabetes
                3. Identification of different penguin species
            2. Fundamentally, a machine learning model is a software application that encapsulates a **function** to calculate an output value based on **one or more input values**. The process of defining that function is known as **training**. After the function has been defined, you can use it to predict new values in a process called **inferencing**.
                1. The **training data** consists of past observations. In most cases, the observations include the **observed attributes** or **features** of the thing being observed, and the known value of the thing you want to train a model to **predict** (known as the **label**).
                2. In mathematical terms, you'll often see the **features referred to using the shorthand variable name *x*,** and the **label referred to as *y***. Usually, an observation consists of multiple feature values, so ***x*** is actually a *vector* (an array with multiple values), like this: ***[x1,x2,x3,...]***.
                3. An **algorithm** is applied to the data to try to determine a **relationship between the features and the label**, and generalize that relationship as a calculation that can be performed on ***x*** to calculate ***y***. The specific algorithm used depends on the kind of predictive problem you're trying to solve (more about this later), but the basic principle is to try to *fit* a function to the data, in which the values of the features can be used to calculate the label.
                4. The result of the algorithm is a *model* that encapsulates the calculation derived by the algorithm as a *function* - let's call it ***f***. In mathematical notation: y = f(x)
                5. Now that the **training phase** is complete, the trained model can be used for **inferencing**. The model is essentially a software program that encapsulates the function produced by the training process. You can input a set of feature values, and receive as an output a prediction of the corresponding label. Because the output from the model is a prediction that was calculated by the function, and not an observed value, you'll often see the output from the function shown as ***ŷ*** (which is rather delightfully verbalized as "y-hat").
            3. **Types of machine learning**
                1. **Supervised machine learning**
                    1. **Regression**
                    2. **Classification**
                        1. **Binary classification**
                        2. **Multiclass classification**
                2. **Unsupervised machine learning**
                    1. **Clustering**
            4. **Regression** - Regression models are trained to **predict numeric label values** based on training data that includes both **features and known labels**. 
                1. Split the training data (randomly) to create a dataset with which to train the model while holding back a subset of the data that you'll use to validate the trained model.
                2. Use an algorithm to fit the training data to a model. In the case of a regression model, use a regression algorithm such as **linear regression.**
                3. Use the validation data you held back to test the model by predicting labels for the features.
                4. Compare the known **actual** labels in the validation dataset to the labels that the model predicted. Then aggregate the differences between the **predicted** and **actual** label values to calculate a metric that indicates how accurately the model predicted for the validation data.
            5. **Evaluating a regression model**
                1. The predicted labels are calculated by the model so they're on the function line, but there's some variance between the ***ŷ*** values calculated by the function and the actual ***y*** values from the validation dataset; which is indicated on the plot as a line between the ***ŷ*** and ***y*** values that shows how far off the prediction was from the actual value
            6. **Regression evaluation metrics**
                1. **Mean Absolute Error (MAE)**
                2. **Mean Squared Error (MSE)**
                3. **Root Mean Squared Error (RMSE)**
                4. **Coefficient of determination (R2)**
            7. **Binary classification**
            8. **Binary classification evaluation metrics.** This visualization is called a *confusion matrix*, and it shows the prediction totals where:
                1. ŷ=0 and y=0: *True negatives* (TN)
                2. ŷ=1 and y=0: *False positives* (FP)
                3. ŷ=0 and y=1: *False negatives* (FN)
                4. ŷ=1 and y=1: *True positives* (TP)
                5. **Accuracy, Recall, Precision, F1-score**
            9. **Multiclass classification**
                1. One-vs-Rest (OvR) algorithms
                2. Multinomial algorithms
            10. **Clustering -**  This kind of machine learning is considered unsupervised because it doesn't make use of previously known label values to train a model. In a clustering model, the label is the cluster to which the observation is assigned, based only on its features.
            11. **Evaluating a clustering model**
                1. **Average distance to cluster center**: How close, on average, each point in the cluster is to the centroid of the cluster.
                2. **Average distance to other center:** How close, on average, each point in the cluster is to the centroid of all other clusters.
                3. **Maximum distance to cluster center:** The furthest distance between a point in the cluster and its centroid.
                4. **Silhouette**: A value between -1 and 1 that summarizes the ratio of distance between points in the same cluster and points in different clusters (The closer to 1, the better the cluster separation).
            12. **Deep learning -** Deep learning is an advanced form of machine learning that tries to emulate the way the human brain learns. The key to deep learning is the creation of an artificial neural network that simulates electrochemical activity in biological neurons by using mathematical functions
            13. **Azure Machine Learning -** The primary resource required for Azure Machine Learning is an *Azure Machine Learning workspace*, which you can provision in an Azure subscription.
            14. **Azure Machine Learning studio**
                1. Import and explore data.
                2. Create and use compute resources.
                3. Run code in notebooks.
                4. Use visual tools to create jobs and pipelines.
                5. Use automated machine learning to train models.
                6. View details of trained models, including evaluation metrics, responsible AI information, and training parameters.
                7. Deploy trained models for on-request and batch inferencing.
                8. Import and manage models from a comprehensive model catalog.
        - **Knowledge check**
            1. You want to create a model to predict the cost of heating an office building based on its size in square feet and the number of employees working there. What kind of machine learning problem is this? - **Regression. Correct. Regression models predict numeric values.**
            2. You need to evaluate a classification model. Which metric can you use? - **Precision. Correct. Precision is a useful metric for evaluating classification models.**
            3. In deep learning, what is the purpose of a loss function? - **To evaluate the aggregate difference between predicted and actual label values. Correct. A loss function determines the overall variance, or loss, between predicted and actual label values.**
            4. What does automated machine learning in Azure Machine Learning enable you to do? - **Automatically run multiple training jobs using different algorithms and parameters to find the best model. Correct. Automated machine learning runs multiple training jobs, varying algorithms and parameters, to find the best model for your data.**
    - **Fundamentals of Azure AI services**
        - **Theory**
            1. Azure AI services
                1. **The Azure AI Content Safety service** can be used to detect harmful content within text or images, including violent or hateful content, and report on its severity. 
                2. **The Azure AI Language service** can be used to summarize text, classify information, or extract key phrases.
                3. **The Azure AI Speech service** provides powerful speech to text and text to speech capabilities, allowing speech to be accurately transcribed into text, or text to natural sounding voice audio.
            2. Azure AI services are based on three principles that dramatically improve speed-to-market:
                1. Prebuilt and ready to use
                2. Accessed through APIs
                3. Available on Azure
            3. **Create Azure AI service resources**
                1. **Multi-service resource**: a resource created in the Azure portal that provides access to multiple Azure AI services with a single key and endpoint. Use the resource **Azure AI services** when you need several AI services or are exploring AI capabilities. When you use an Azure AI services resource, all your AI services are billed together.
                2. **Single-service resources**: a resource created in the Azure portal that provides access to a single Azure AI service, such as Speech, Vision, Language, etc. Each Azure AI service has a unique key and endpoint. These resources might be used when you only require one AI service or want to see cost information separately.
            4. To create an Azure AI services resource, sign in to the [Azure portal](https://portal.azure.com/) with Contributor access and select **Create a resource**. To create a **multi-services resource search for Azure AI services** in the marketplace. To create a single-service resource, search for the **specific Azure AI service such as Face, Language, or Content Safety,** and so on. 
            5. **Using service studio interfaces**
                1. Studio interfaces provide a friendly user interface to explore Azure AI services. There are different studios for different Azure AI services, such as **Vision Studio, Language Studio, Speech Studio, and the Content Safety Studio.** 
            6. **Endpoint** - The endpoint describes how to reach the AI service resource instance that you want to use, in a similar way to the way a URL identifies a web site. 
            7. **Resource key** - The resource key protects the privacy of your resource. To ensure this is always secure, the key can be changed periodically.
            8. When you write code to access the AI service, the keys and endpoint must be included in the **authentication header**. The authentication header sends an authorization key to the service to confirm that the application can use the resource.
        - **Knowledge check**
            1. An application requires three separate AI services. To see the cost for each separately, what type of resource(s) should be created? - **A single-service resource for each AI service. Correct. Create a single-service resource for each AI service to see costs separately for each resource.**
            2. After logging into one of the Azure studios, what is one task to complete to begin using the studio? - **Associate a resource with the studio. Correct. To explore the capabilities of the service in the studio, you must first associate the resource with the studio.**
            3. What is an Azure AI services resource? - **A bundle of several AI services in one resource. Correct. An Azure AI services resource is a bundle of several AI services in one resource.**
- **Microsoft Azure AI Fundamentals: Computer Vision**
    - **Fundamentals of Computer Vision**
        - **Theory**
            1. Computer vision is one of the core areas of artificial intelligence (AI), and focuses on creating solutions that enable AI applications to "see" the world and make sense of it. Computer vision is built on the analysis and manipulation of numeric pixel values in images.
            2. The goal of computer vision is often to extract meaning, or at least actionable insights, from images; which requires the creation of machine learning models that are trained to recognize features based on large volumes of existing images.
            3. Convolutional neural networks (CNNs). CNNs use filters to extract numeric feature maps from images, and then feed the feature values into a deep learning model to generate a label prediction. For example, in an *image classification* scenario, the label represents the main subject of the image (in other words, what is this an image of?). You might train a CNN model with images of different kinds of fruit (such as apple, banana, and orange) so that the label that is predicted is the type of fruit in a given image.
            4. Images with known labels (for example, 0: apple, 1: banana, or 2: orange) are fed into the network to train the model.
            5. One or more layers of filters is used to extract features from each image as it is fed through the network. The filter kernels start with randomly assigned weights and generate arrays of numeric values called feature maps.
            6. The feature maps are flattened into a single dimensional array of feature values.
            7. The feature values are fed into a fully connected neural network.
            8. The output layer of the neural network uses a softmax or similar function to produce a result that contains a probability value for each possible class, for example [0.2, 0.5, 0.3].
            9.  The difference between the predicted and actual class scores is used to calculate the *loss* in the model, and the weights in the fully connected neural network and the filter kernels in the feature extraction layers are modified to reduce the loss.
            10. Transformers work by processing huge volumes of data, and encoding language *tokens* (representing individual words or phrases) as vector-based *embeddings* (arrays of numeric values). You can think of an embedding as representing a set of dimensions that each represent some semantic attribute of the token. The embeddings are created such that tokens that are commonly used in the same context are closer together dimensionally than unrelated words.
            11. The Microsoft *Florence* model is just such a model. Trained with huge volumes of captioned images from the Internet, it includes both a language encoder and an image encoder. Florence is an example of a *foundation* model. In other words, a pre-trained general model on which you can build multiple *adaptive* models for specialist tasks. For example, you can use Florence as a foundation model for adaptive models that perform:
                1. I*mage classification*: Identifying to which category an image belongs.
                2. *Object detection*: Locating individual objects within an image.
                3. C*aptioning*: Generating appropriate descriptions of images.
                4. T*agging*: Compiling a list of relevant text tags for an image.
            12. Microsoft's Azure AI Vision service provides prebuilt and customizable computer vision models that are based on the Florence foundation model and provide various powerful capabilities. 
            13. **Azure AI Vision**: A specific resource for the Azure AI Vision service. Use this resource type if you don't intend to use any other Azure AI services, or if you want to track utilization and costs for your Azure AI Vision resource separately.
            14. **Azure AI services:** A general resource that includes Azure AI Vision along with many other Azure AI services; such as Azure AI Language, Azure AI Custom Vision, Azure AI Translator, and others. Use this resource type if you plan to use multiple AI services and want to simplify administration and development.
            15. Azure AI Vision supports multiple image analysis capabilities, including:
                1. Optical character recognition (OCR) - extracting text from images.
                2. Generating captions and descriptions of images.
                3. Detection of thousands of common objects in images.
                4. Tagging visual features in images
            16. **Azure AI vision studio**
            17. Azure AI Vision builds **custom models** on the pre-trained foundation model, meaning that you can train sophisticated models by using relatively few training images.
                1. **Image classification**
                2. **Object detection**
        - **Knowledge check**
            1. Computer vision is based on the manipulation and analysis of what kinds of values in an image? - **Pixels. Correct. Pixels are numeric values that represent shade intensity for points in the image.**
            2. You want to use the Azure AI Vision service to analyze images. You also want to use the Azure AI Language service to analyze text. You want developers to require only one key and endpoint to access all of your services. What kind of resource should you create in your Azure subscription? - **Azure AI services. Correct. An Azure AI Services resource supports both Azure AI Vision and Azure AI Language.**
            3. You want to use the Azure AI Vision service to identify the location of individual items in an image. Which of the following features should you retrieve? - **Objects. Correct. Azure AI Vision returns objects with a bounding box to indicate their location in the image.**
    - **Fundamentals of Facial Recognition**
        - **Theory**
            1. Face detection and analysis is an area of artificial intelligence (AI) which uses algorithms to locate and analyze human faces in images or video content. 
            2. While Azure AI Vision and Azure AI Video Indexer offer face detection features, the Face service and associated APIs provide more capabilities. Which you should choose will ultimately depend on the insights you want to gain from facial detection. If you want to look for data around facial landmarks, the Face resource is the best choice.
            3. There are many applications for face detection, analysis, and recognition. For example,
                1. **Security** - facial recognition can be used in building security applications, and increasingly it is used in smart phones operating systems for unlocking devices.
                2. **Social media** - facial recognition can be used to automatically tag known friends in photographs.
                3. **Intelligent monitoring** - for example, an automobile might include a system that monitors the driver's face to determine if the driver is looking at the road, looking at a mobile device, or shows signs of tiredness.
                4. **Advertising** - analyzing faces in an image can help direct advertisements to an appropriate demographic audience.
                5. **Missing persons** - using public cameras systems, facial recognition can be used to identify if a missing person is in the image frame.
                6. **Identity validation** - useful at ports of entry kiosks where a person holds a special entry permit.
            4. **Face detection** involves identifying regions of an image that contain a human face, typically by returning *bounding box* coordinates that form a rectangle around the face, like this:
            5. With **Face analysis**, facial features can be used to train machine learning models to return other information, such as facial features such as nose, eyes, eyebrows, lips, and others.
            6. A further application of facial analysis is to train a machine learning model to identify known individuals from their facial features. This is known as ***facial recognition***, and uses multiple images of an individual to train the model. 
            7. **Azure AI Face service**
            8. **Azure AI Vision**, which offers face detection and some basic face analysis, such as returning the bounding box coordinates around an image.
            9. **Azure AI Video Indexer**, which you can use to detect and identify faces in a video.
            10. **Azure AI Face**, which offers pre-built algorithms that can detect, recognize, and analyze faces.
            11. **Face service - Accessories, Blur, Exposure, Glasses, Head pose, Mask, Noise, Occlusion**
            12. Limited Access policy 
                1. The ability to compare faces for similarity.
                2. The ability to identify named individuals in an image.
            13. **Face:** Use this specific resource type if you don't intend to use any other Azure AI services, or if you want to track utilization and costs for Face separately.
            14. **Azure AI services:** A general resource that includes Azure AI Face along with many other Azure AI services such as Azure AI Content Safety, Azure AI Language, and others. Use this resource type if you plan to use multiple Azure AI services and want to simplify administration and development.
            15. **Tips for more accurate results**
                1. Image format - supported images are JPEG, PNG, GIF, and BMP.
                2. File size - 6 MB or smaller.
                3. Face size range - from 36 x 36 pixels up to 4096 x 4096 pixels. Smaller or larger faces will not be detected.
                4. Other issues - face detection can be impaired by extreme face angles, extreme lighting, and occlusion (objects blocking the face such as a hand).
            16. **Vision studio**
        - **Knowledge check**
            1. How does the Face service indicate the location of faces in images? - **A set of coordinates for each face, defining a rectangular bounding box around the face. Correct: The locations of detected faces are indicated by coordinates for a rectangular bounding box**
            2. What is one aspect that might impair facial detection? - **Extreme angles. Correct: Best results are obtained when the faces are full-frontal or as near as possible to full-frontal**
            3. What two actions are required to try out the capabilities of the Face service? - **Create a Face resource, and open Vision Studio. Correct: The Face resource has face detections capabilities, and can be used in Vision Studio to understand its capabilities.**
    - **Fundamentals of optical character recognition**
        - **Theory**
            1. **Optical character recognition (OCR),** the capability for artificial intelligence (AI) to process words in images into machine-readable text. Optical character recognition (OCR) has been around for a long time. The ability to do the same extraction from images is where the Read API can help. The Read API provides the ability to extract large amounts of typewritten or handwritten text from images.
            2. OCR is the foundation of processing text in images and uses machine learning models that are trained to recognize individual shapes as letters, numerals, punctuation, or other elements of text. 
            3. **Azure AI Vision service** has the ability to extract machine-readable text from images. 
                1. **Azure AI Vision's Read API** is the OCR engine that powers text extraction from images, PDFs, and TIFF files. The Read API, otherwise known as *Read OCR engine*, uses the latest recognition models and is optimized for images that have a significant amount of text or have considerable visual noise. It can automatically determine the proper recognition model to use taking into consideration the number of lines of text, images that include text, and handwriting. Calling the Read API returns results arranged into the following hierarchy:
                    1. **Pages** - One for each page of text, including information about the page size and orientation.
                    2. **Lines** - The lines of text on a page.
                    3. **Words** - The words in a line of text, including the bounding box coordinates and text itself.
                2. **OCR for images** is optimized for general, non-document images that makes it easier to embed OCR in your user experience scenarios. The OCR engine takes in an image file and identifies bounding boxes, or coordinates, where items are located within an image. In OCR, the model identifies bounding boxes around anything that appears to be text in the image.
            4. **Azure AI Vision:** A specific resource for vision services. Use this resource type if you don't intend to use any other AI services, or if you want to track utilization and costs for your AI Vision resource separately.
            5. **Azure AI services:** A general resource that includes Azure AI Vision along with many other Azure AI services such as Azure AI Language, Azure AI Speech, and others. Use this resource type if you plan to use multiple Azure AI services and want to simplify administration and development.
            6. There are several ways to use Azure AI Vision's Read API:
                1. Vision Studio
                2. REST API
                3. Software Development Kits (SDKs): Python, C#, JavaScript
            7. **Vision studio**
        - **Knowledge check**
            1. You want to extract text from images and then use Azure AI Language to analyze the text. You want developers to require only one key and endpoint to access all of your services. What kind of resource should you create in your Azure subscription? - **Azure AI services. Correct. An Azure AI services resource supports both Azure AI Vision for text extraction, and Azure AI Language for text analytics.**
            2. You plan to use Azure AI Vision's Read API. What results can the Read API provide? - **Results arranged in pages, lines, and words. Correct: The Read API takes an image and extracts the words, organizing the results by page and line.**
- **Microsoft Azure AI Fundamentals: Natural Language Processing**
    - **Fundamentals of Text Analysis with the Language Service**
        - **Theory**
            1. Natural language processing (NLP), an area within AI that deals with understanding written or spoken language, and responding in kind.
            2. Text analysis describes NLP processes that extract information from unstructured text.
            3. Natural language processing might be used to create:
                1. A  social media feed analyzer that detects sentiment for a product marketing campaign.
                2. A document search application that summarizes documents in a catalog.
                3. An application that extracts brands and company names from text.
            4. Azure AI Language is a cloud-based service that includes features for understanding and analyzing text. Azure AI Language includes various features that support sentiment analysis, key phrase identification, text summarization, and conversational language understanding.
            5. At the heart of these models is the encoding of language tokens as vectors (multi-valued arrays of numbers) known as embeddings.
            6. Common NLP tasks supported by language models include:
                1. Text analysis, such as extracting key terms or identifying named entities in text.
                2. Sentiment analysis and opinion mining to categorize text as positive or negative.
                3. Machine translation, in which text is automatically translated from one language to another.
                4. Summarization, in which the main points of a large body of text are summarized.
                5. Conversational AI solutions such as bots or digital assistants in which the language model can interpret natural language input and return an appropriate response.
            7. Named entity recognition identifies people, places, events, and more. This feature can also be customized to extract custom categories.
            8. Entity linking identifies known entities together with a link to Wikipedia.
            9. Personal identifying information (PII) detection identifies personally sensitive information, including personal health information (PHI).
            10. Language detection identifies the language of the text and returns a language code such as "en" for English.
            11. Sentiment analysis and opinion mining identifies whether text is positive or negative.
            12. Summarization summarizes text by identifying the most important information.
            13. Key phrase extraction lists the main concepts from unstructured text.
            14. A **Language** resource or An **Azure AI services** resource. Language studio
        - **Knowledge check**
            1. You want to use Azure AI Language to determine the key talking points in a text document. Which feature of the service should you use? - **Key phrase extraction. Correct. Key phrases can be used to identify the main talking points in a text document.**
            2. You use Azure AI Language to perform sentiment analysis on a sentence. The confidence scores .04 positive, .36 neutral, and .60 negative are returned. What do these confidence scores indicate about the sentence sentiment? - **The document is negative. Correct. The sentiment is most likely the type with the highest confidence score, in this case .6 negative.**
            3. When might you see NaN returned for a score in language detection? **- When the language is ambiguous Correct. The service will return NaN when it can't determine the language in the provided text.**
    - **Fundamentals of question answering with the Language Service**
        - **Theory**
            1. Conversational AI describes solutions that enable a dialog between an AI agent and a human. Generically, conversational AI agents are known as bots. People can engage with bots through channels such as web chat interfaces, email, social media platforms, and more.
            2. Azure AI Language: includes a custom question answering feature that enables you to create a knowledge base of question and answer pairs that can be queried using natural language input.
            3. Azure AI Bot Service: provides a framework for developing, publishing, and managing bots on Azure.
            4. Language resource and Language studio.
            5. Creating a custom question answering knowledge base and Build a bot with Azure AI Bot Service
            6. Question answering AI service
        - **Knowledge check**
            1. Your organization has an existing frequently asked questions (FAQ) document. You need to create a knowledge base that includes the questions and answers from the FAQ with the least possible effort. What should you do?  - **Import the existing FAQ document into a new knowledge base. Correct. You can import question and answer pairs from an existing FAQ document into a question answering knowledge base.**
            2. You want to create a knowledge base for your organization’s bot service. Which Azure AI service is best suited to creating a knowledge base? **- Question Answering. Correct. Question Answering is part of the Azure AI Language service and enables you to create a knowledge base of question and answer pairs**
    - **Fundamentals of conversational language understanding**
        - **Theory**
            1. One example using conversational language understanding is an application that's able to turn devices on and off based on speech. Many types of tasks involving command and control, end-to-end conversation, and enterprise support can be completed with Azure AI Language's conversational language understanding feature.
            2. Three core concepts: utterances, entities, and intents.
            3. An utterance is an example of something a user might say, and which your application must interpret. For example, when using a home automation system, a user might use the following utterances: "Switch the fan on.”
            4. An entity is an item to which an utterance refers. For example, fan and light in the following utterances: "Switch the fan on.”
            5. An intent represents the purpose, or goal, expressed in a user's utterance. For example, for both of the previously considered utterances, the intent is to turn a device on; so in your conversational language understanding application, you might define a TurnOn intent that is related to these utterances.
            6. Of special interest is the None intent. The None intent is considered a fallback, and is typically used to provide a generic response to users when their requests don't match any other intent.
            7. Azure AI Language: A resource that enables you to build apps with industry-leading natural language understanding capabilities without machine learning expertise. You can use a language resource for authoring and prediction.
            8. Azure AI services: A general resource that includes conversational language understanding along with many other Azure AI services. You can only use this type of resource for prediction.
            9. Authoring and Predicting
            10. Client applications can use the model by connecting to the endpoint for the prediction resource, specifying the appropriate authentication key; and submit user input to get predicted intents and entities. The predictions are returned to the client application, which can then take appropriate action based on the predicted intent.
        - **Knowledge check**
            1. You need to provision an Azure resource that will be used to author a new conversational language understanding application. What kind of resource should you create? - **Azure AI Language.** **Correct. To author a conversational language understanding model, you need an Azure AI Language resource.**
            2. You are authoring a conversational language understanding application to support an international clock. You want users to be able to ask for the current time in a specified city, for example "What is the time in London?". What should you do? - **Define a "city" entity and a "GetTime" intent with utterances that indicate the city entity. Correct. The intent encapsulates the task (getting the time) and the entity specifies the item to which the intent is applied (the city).**
            3. You have published your conversational language understanding application. What information does a client application developer need to get predictions from it? - **The endpoint and key for the application's prediction resource. Correct. Client applications must connect to the endpoint of the prediction resource, specifying an associated authentication key.**
    - **Fundamentals of Azure AI Speech**
        - **Theory**
            1. Speech recognition - the ability to detect and interpret spoken input
                1. An acoustic model that converts the audio signal into phonemes (representations of specific sounds).
                2. A language model that maps phonemes to words, usually using a statistical algorithm that predicts the most probable sequence of words based on the phonemes.
            2. Speech synthesis - the ability to generate spoken output. A speech synthesis solution typically requires the following information:
                1. The text to be spoken
                2. The voice to be used to vocalize the speech
            3. Azure AI Speech provides speech to text and text to speech capabilities through speech recognition and synthesis. You can use prebuilt and custom Speech service models for a variety of tasks, from transcribing audio to text with high accuracy, to identifying speakers in conversations, creating custom voices, and more
            4. Microsoft Azure offers both speech recognition and speech synthesis capabilities through Azure AI Speech service, which includes the following application programming interfaces (APIs):
                1. The Speech to text API - You can use Azure AI Speech to text API to perform real-time or batch transcription of audio into a text format. The audio source for transcription can be a real-time audio stream from a microphone or an audio file.
                2. The Text to speech API
            5. A Speech resource - choose this resource type if you only plan to use Azure AI Speech, or if you want to manage access and billing for the resource separately from other services.
            6. An Azure AI services resource - choose this resource type if you plan to use Azure AI Speech in combination with other Azure AI services, and you want to manage access and billing for these services together.
            7. Speech studio
        - **Knowledge check**
            1. You plan to build an application that uses Azure AI Speech to transcribe audio recordings of phone calls into text, and then submit the transcribed text to Azure AI Language to extract key phrases. You want to manage access and billing for the application services with a single Azure resource. Which type of Azure resource should you create? - **Azure AI services Correct. This resource would support both the Azure AI Speech and Azure AI Language services.**
            2. You want to use Azure AI Speech service to build an application that reads incoming email message subjects aloud. Which API should you use? - **Text to speech. Correct. The Text to speech API converts text to audible speech.**
- **Microsoft Azure AI Fundamentals: Document Intelligence and Knowledge Mining**
    - **Fundamentals of Azure AI Document Intelligence**
        - **Theory**
            1. Document intelligence describes AI capabilities that support processing text and making sense of information in text. As an extension of optical character recognition (OCR), document intelligence takes the next step a person might after reading a form or document. It automates the process of extracting, understanding, and saving the data in text.
            2. Azure AI Document Intelligence supports features that can analyze documents and forms with prebuilt and custom models.
            3. Document intelligence relies on machine learning models that are trained to recognize data in text. The ability to extract text, layout, and key-value pairs are known as *document analysis*. Document analysis provides locations of text on a page identified by bounding box coordinates.
            4. Azure AI Document Intelligence consists of features grouped by model type:
                1. Prebuilt models - pretrained models that have been built to process common document types such as invoices, business cards, ID documents, and more. These models are designed to recognize and extract specific fields that are important for each document type.
                2. Custom models - can be trained to identify specific fields that are not included in the existing pretrained models.
                3. Document analysis - general document analysis that returns structured data representations, including regions of interest and their inter-relationships.
            5. Receipt model - The receipt model has been trained to recognize data on several different receipt types, such as thermal receipts (printed on heat-sensitive paper), hotel receipts, gas receipts, credit card receipts, and parking receipts. Fields recognized include:
                1. Name, address, and telephone number of the merchant
                2. Date and time of the purchase
                3. Name, quantity, and price of each item purchased
                4. Total, subtotals, and tax values
            6. It processes receipts by:
                1. Matching field names to values
                2. Identifying tables of data
                3. Identifying specific fields, such as dates, telephone numbers, addresses, totals, and others
            7. Document intelligence studio - a user interface for testing document analysis, prebuilt models, and creating custom models.
        - **Knowledge check**
            1. You plan to use Azure AI Document Intelligence's prebuilt receipt model. Which kind of Azure resource should you create? **Azure AI Document Intelligence or Azure AI services resource. Correct: Both the Azure AI Document Intelligence resource and the Azure AI services resource provide access to Azure AI Document Intelligence.**
            2. You are using the Azure AI Document Intelligence service to analyze receipts. Which field types does the service recognize? **Merchant name and address. Correct: The merchant name and address can be identified using the receipt model.**
            3. What is required to use the receipt analyzer service in Azure AI Document Intelligence? - **Create an Azure AI Document Intelligence resource. Correct: The receipt analyzer model is available as a service when you create an Azure AI Document Intelligence resource.**
    - **Fundamentals of Knowledge Mining and Azure AI Search**
        - **Theory**
            1. Knowledge mining is the term used to describe solutions that involve extracting information from large volumes of often unstructured data. One of these knowledge mining solutions is Azure AI Search, a cloud search service that has tools for building user-managed indexes. The indexes can be used for internal only use, or to enable searchable content on public-facing internet assets.
            2. Importantly, Azure AI Search can utilize the built-in capabilities of Azure AI services such as image processing, content extraction, and natural language processing to perform knowledge mining of documents
            3. Azure AI Search is a Platform as a Service (PaaS) solution.
            4. Apache Lucene, an open-source software library. 
            5. Azure AI Search comes with the following features:
                1. Data from any source: accepts data from any source provided in JSON format, with auto crawling support for selected data sources in Azure.
                2. Full text search and analysis: offers full text search capabilities supporting both simple query and full Lucene query syntax.
                3. AI powered search: has Azure AI capabilities built in for image and text analysis from raw content.
                4. Multi-lingual offers linguistic analysis for 56 languages to intelligently handle phonetic matching or language-specific linguistics. Natural language processors available in Azure AI Search are also used by Bing and Office.
                5. Geo-enabled: supports geo-search filtering based on proximity to a physical location.
                6. Configurable user experience: has several features to improve the user experience including autocomplete, autosuggest, pagination, and hit highlighting.
            6. Elements of search solution
                1. Data source that contains the data artifacts you want to search. The data format that Azure AI Search supports is JSON. Regardless of where your data originates, if you can provide it as a JSON document, the search engine can index it.
            7. Skillset to define an enrichment pipeline
            8. Built in skills - Natural language processing skills and Image processing skills
            9. Indexes - Index schema and Index attributes
            10. Indexer - Export data in any format to JSON. Azure AI Search lets you create and load JSON documents into an index with two approaches:
                1. **Push method**: JSON data is pushed into a search index via either the REST API or the .NET SDK. Pushing data has the most flexibility as it has no restrictions on the data source type, location, or frequency of execution.
                2. **Pull method**: Search service indexers can pull data from popular Azure data sources, and if necessary, export that data into JSON if it isn't already in that form
            11. Persist enriched data in a knowledge store.
            12. Supported data sources include: Cosmos DB (SQL API), Azure SQL (database, managed instance, and SQL Server on an Azure VM), Azure Storage (Blob Storage, Table Storage, ADLS Gen2)
            13. Data Source: Persists connection information to source data, including credentials. A data source object is used exclusively with indexers.
            14. Index: Physical data structure used for full text search and other queries.
            15. Indexer: A configuration object specifying a data source, target index, an optional AI skillset, optional schedule, and optional configuration settings for error handling and base-64 encoding.
            16. Skillset: A complete set of instructions for manipulating, transforming, and shaping content, including analyzing and extracting information from image files. Except for very simple and limited structures, it includes a reference to an Azure AI services resource that provides enrichment.
            17. Knowledge store: Stores output from an AI enrichment pipeline in tables and blobs in Azure Storage for independent analysis or downstream processing
        - **Knowledge check**
            1. Which data format is accepted by Azure AI Search when you're pushing data to the index? - **JSON. Correct. Azure AI Search can index JSON documents. JSON is also used to define index schemas, indexers, and data source objects.**
            2. Which explanation best describes an indexer and an index? - **An indexer converts documents into JSON and forwards them to a search engine for indexing. Correct. An indexer serializes a source document into JSON before passing it to a search engine for indexing. An indexer automates several steps of data ingestion, reducing the amount of code you need to write.**
            3. If you set up a search index without including a skillset, which would you still be able to query? - **Text content. Correct. Azure AI Search is used for full text search over indexes containing alphanumeric content.**
- **Microsoft Azure AI Fundamentals: Generative AI**
    - **Fundamentals of Generative AI**
        - **Theory**
            1. Generative AI describes a category of capabilities within AI that create original content. Example: Microsoft copilot, a chatbot companion to browse the web more effectively. Generative AI applications take in natural language input, and return appropriate responses in a variety of formats such as natural language, images, or code.
            2. Language models are specialized type of machine learning model that you can use to perform natural language processing (NLP) tasks, including 
                1. Determine sentiment or otherwise classifying natural language text
                2. Summarizing text
                3. Comparing multiple text sources for semantic similarity
                4. Generating new natural language
            3. Transformer model - Large language models architecture.
            4. Transformer models are trained with large volumes of text, enabling them to represent the semantic relationships between words and use those relationships to determine (or predict) probable sequences of text that make sense.
            5. Encoder block creates semantic relationships of the training vocabulary. Decode block generates new languages sequences.
            6. Transformer model such as GPT-4 (the model behind ChatGPT and Bing) is designed to take in a text input (called a prompt) and generate a syntactically correct output (called a completion). 
            7. In effect, the “magic” of the model is that it has the ability to string a coherent sentence together. This ability doesn't imply any “knowledge” or “intelligence” on the part of the model; just a large vocabulary and the ability to generate meaningful sequences of words.
            8. Azure OpenAI service - Large language model in Azure. This offers the benefit of cutting-edge language models like the generative pre-trained transformer (GPT) collection of models (on which ChatGPT and Microsoft's own generative AI services are based) as well as the DALL-E model for image generation.
            9. Language models: Large language models and small language models
            10. Copilots - Generative AI assistants that are integrated into applications often as chat interfaces. In general, you can categorize industry and personal Copilot adoption into three buckets: off-the-shelf use, extending Microsoft Copilot, and custom development.
                1. Microsoft copilot for web browsing
                2. Copilot for Microsoft 365 (Word, Powerpoint, and Outlook)
                3. Copilot in Dynamics 365 customer service, sales, supply chain
                4. Copilot in Microsoft fabric, Power BI
                5. Microsofy copilot for security, Azure
                6. Github copilot
            11. Considerations for copliot prompts
            12. Copilot studio and Azure AI studio
        - **Knowledge check**
            1. What are Large Language Models? - **Models that use deep learning to process and understand natural language on a massive scale. Correct. Large language models use deep learning to process and understand natural language on a massive scale.**
            2. Which Microsoft Copilot should a customer support agent use to research and resolve a support issue? - **Microsoft Copilot for Dynamics 365 Customer Service. Customer service agents can use Copilot in Dynamics 365 Customer Service to analyze support tickets, research similar issues, find resolutions, and communicate them to users with only a few clicks and prompts.**
            3. Which tool should a professional developer use to build a custom copilot and deploy it as a service endpoint in Azure? - **Microsoft Azure AI Studio. Microsoft Azure AI Studio is designed as a unified development portal for professional software developers to allow for full customization of language models.**
    - **Fundamentals of Azure OpenAI Service**
        - **Theory**
            1. Capabilities of Open AI models: Generate natural language, Generating code, and Generating images
            2. Artificial intelligence imitates human behavior by relying on machine to learn and execute tasks without explicit directions on what to output
            3. Machine learning algorithms take in data like weather conditions and fit models to the data, to make predictions like how much money a store might make in a given day
            4. Deep learning model use layers of algorithms in the form of artificial neural networks to return results for more complex use cases
            5. Generative AI models can produce new content based on what is described in the input. The OpenAI models are a collection of generative AI models that can produce language, code, and images.
            6. Azure OpenAI supports many generative AI workloads such as:
                1. Generating Natural Language
                    1. Text completion: generate and edit text
                    2. Embeddings: search, classify, and compare text
                2. Generating Code: generate, edit, and explain code
                3. Generating Images: generate and edit images
            7. Azure AI Language service can be used for widely known use-cases that require minimal tuning (the process of optimizing a model's performance). Azure OpenAI Service may be more beneficial for use-cases that require highly customized generative models, or for exploratory research.
            8. Currently you need to apply for access to Azure OpenAI. Once granted access, you can use the service by creating an Azure OpenAI resource, like you would for other Azure services.
            9. Azure OpenAI studio - you can build AI models and deploy them for public consumption in software applications. 
                1. **GPT-4** models that represent the latest generative models for natural language and code.
                2. **GPT-3.5** models that can generate natural language and code responses based on prompts.
                3. **Embeddings** models that convert text to numeric vectors for analysis - for example comparing sources of text for similarity.
                4. **DALL-E** models that generate images based on natural language descriptions.
            10. Completion playgrounds and chat playgrounds
            11. The combination of the front and back end can be described as a chatbot.
            12. GitHub copilot - OpenAI partnered with GitHub to create GitHub Copilot, which they call an AI pair programmer. GitHub Copilot integrates the power of OpenAI Codex into a plugin for developer environments like Visual Studio Code. Once the plugin is installed and enabled, you can start writing your code, and GitHub Copilot starts automatically suggesting the remainder of the function based on code comments or the function name. For example, we have only a function name in the file, and the gray text is automatically suggested to complete it. GitHub Copilot offers multiple suggestions for code completion, which you can tab through using keyboard shortcuts. When given informative code comments, it can even suggest a function name along with the complete function code.
            13. Image capabilities generally fall into the three categories of image creation, editing an image, and creating variations of an image. Access to DALL-E is currently granted on an invite basis only.
            14. Usage of Azure OpenAI should follow the six Microsoft AI principles
                1. **Fairness**: AI systems shouldn't make decisions that discriminate against or support bias of a group or individual.
                2. **Reliability and Safety**: AI systems should respond safely to new situations and potential manipulation.
                3. **Privacy and Security**: AI systems should be secure and respect data privacy.
                4. **Inclusiveness**: AI systems should empower everyone and engage people.
                5. **Accountability**: People must be accountable for how AI systems operate.
                6. **Transparency**: AI systems should have explanations so users can understand how they're built and used.
                
        - **Knowledge check**
            1. How are ChatGPT, OpenAI, and Azure OpenAI related? - **OpenAI is a research company that developed ChatGPT, a chatbot that uses generative AI models. Azure OpenAI provides access to many of OpenAI's AI models.**
            2. You would like to summarize a paragraph of text. Which generative AI model family would you use to solve for this workload? - **GPT**
            3. What is one action Microsoft takes to support ethical AI practices in Azure OpenAI? - **Provides Transparency Notes that share how technology is built and asks users to consider its implications.**
    - **Fundamentals of Responsible Generative AI**
        - **Theory**
            1. Four stage process to develop and implement a plan for responsible AI when using generative models
                1. Identify potential harms that are relevant to your planned solution.
                2. Measure the presence of these harms in the outputs generated by your solution.
                3. Mitigate the harms at multiple layers in your solution to minimize their presence and impact, and ensure transparent communication about potential risks to users.
                4. Operate the solution responsibly by defining and following a deployment and operational readiness plan.
            2. Identify potential harms
                1. Identify potential harms
                2. Prioritize identified harms
                3. Test and verify the prioritized harms
                4. Document and share the verified harms
            3. Measure potential harms
                1. Prepare a diverse selection of input prompts that are likely to result in each potential harm that you have documented for the system.
                2. Submit the prompts to the system and retrieve the generated output.
                3. Apply pre-defined criteria to evaluate the output and categorize it according to the level of potential harm it contains. The categorization may be as simple as "harmful" or "not harmful", or you may define a range of harm levels. Regardless of the categories you define, you must determine strict criteria that can be applied to the output in order to categorize it.
            4. Mitigate potential harms
                1. Model
                2. Safety system
                3. Metaprompt and grounding
                4. User experience
            5. Operate a responsible generative AI solution
        - **Knowledge check**
            1. Why should you consider creating an AI Impact Assessment when designing a generative AI solution? - **To document the purpose, expected use, and potential harms for the solution. An AI Impact Assessment guide documents the expected use of the system and helps identify potential harms.**
            2. What capability of Azure OpenAI Service helps mitigate harmful content generation at the Safety System level? - **Content filters. Content filters enable you to suppress harmful content at the Safety System layer.**
            3. Why should you consider a phased delivery plan for your generative AI solution? - **To enable you to gather feedback and identify issues before releasing the solution more broadly. An initial release to a restricted user base enables you to minimize harm by gather feedback and identifying issues before broad release.**